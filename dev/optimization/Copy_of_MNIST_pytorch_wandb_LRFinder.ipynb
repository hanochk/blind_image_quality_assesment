{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MNIST_pytorch_wandb_LRFinder",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJrTcams9S6G"
      },
      "source": [
        "## Imports and Setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HESv1l--3AAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4b58b2-e647-455c-84d9-0d28690f61fa"
      },
      "source": [
        "!pip install wandb -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8MB 32.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 52.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 56.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 12.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg0iv_RPT3YG"
      },
      "source": [
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zcjcQ3-UH2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1358fee2-374f-4ffd-be47-b0ae970c17ae"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RFZ81yY3Rfl"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtakKUgw9Ype"
      },
      "source": [
        "#### For GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiEHlMTh4ecC"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIxBcFVg9clX"
      },
      "source": [
        "## MNIST Hand written Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtM4FMPP3imH"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8luRi8q9pyh"
      },
      "source": [
        "## Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3rki8u__TU4"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1, bias=False)\n",
        "\n",
        "        self.fc1 = nn.Linear(9216, 128, bias=False)\n",
        "        self.fc2 = nn.Linear(128, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Conv 1st Block\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x) \n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x) \n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu5wSfFfln7X"
      },
      "source": [
        "## Learning Rate Finder with W&B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IOI5A_9Xrw0"
      },
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ygEbJZZXQ1"
      },
      "source": [
        "## Reference: https://github.com/davidtvs/pytorch-lr-finder/blob/14abc0b8c3edd95eefa385c2619028e73831622a/torch_lr_finder/lr_finder.py\n",
        "\n",
        "class LRFinder(object):\n",
        "    def __init__(self,model,optimizer,device=None,memory_cache=True,cache_dir=None):\n",
        "        # Check if the optimizer is already attached to a scheduler\n",
        "        self.optimizer = optimizer\n",
        "        self.model = model\n",
        "        self.history = {\"lr\": [], \"loss\": []}\n",
        "        self.best_loss = None\n",
        "        self.device = device    \n",
        "    \n",
        "    def range_test(self,\n",
        "        train_loader,\n",
        "        val_loader=None,\n",
        "        start_lr=None,\n",
        "        end_lr=10,\n",
        "        num_iter=100,\n",
        "        smooth_f=0.05,\n",
        "        diverge_th=8,\n",
        "        accumulation_steps=1,\n",
        "        logwandb=False\n",
        "    ):\n",
        "        # Reset test results\n",
        "        self.history = {\"lr\": [], \"loss\": []}\n",
        "        self.best_loss = None\n",
        "\n",
        "        # Move the model to the proper device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Set the starting learning rate\n",
        "        if start_lr:\n",
        "            self._set_learning_rate(start_lr)\n",
        "\n",
        "        # Initialize the proper learning rate policy\n",
        "        lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
        "        \n",
        "        if smooth_f < 0 or smooth_f >= 1:\n",
        "            raise ValueError(\"smooth_f is outside the range [0, 1]\")\n",
        "\n",
        "        # Create an iterator to get data batch by batch\n",
        "        iter_wrapper = DataLoaderIterWrapper(train_loader)\n",
        "        \n",
        "        for iteration in range(num_iter):\n",
        "            # Train on batch and retrieve loss\n",
        "            loss = self._train_on_batch(iter_wrapper, accumulation_steps)\n",
        "    \n",
        "            # Update the learning rate\n",
        "            lr_schedule.step()\n",
        "            self.history[\"lr\"].append(lr_schedule.get_lr()[0])\n",
        "\n",
        "            # Track the best loss and smooth it if smooth_f is specified\n",
        "            if iteration == 0:\n",
        "                self.best_loss = loss\n",
        "            else:\n",
        "                if smooth_f > 0:\n",
        "                    loss = smooth_f * loss + (1 - smooth_f) * self.history[\"loss\"][-1]\n",
        "                if loss < self.best_loss:\n",
        "                    self.best_loss = loss\n",
        "\n",
        "            # Check if the loss has diverged; if it has, stop the test\n",
        "            self.history[\"loss\"].append(loss)\n",
        "            \n",
        "            if logwandb:\n",
        "              wandb.log({'lr': lr_schedule.get_lr()[0], 'loss': loss})\n",
        "\n",
        "            if loss > diverge_th * self.best_loss:\n",
        "                print(\"Stopping early, the loss has diverged\")\n",
        "                break\n",
        "\n",
        "        print(\"Learning rate search finished\")\n",
        "\n",
        "    def _set_learning_rate(self, new_lrs):\n",
        "        if not isinstance(new_lrs, list):\n",
        "            new_lrs = [new_lrs] * len(self.optimizer.param_groups)\n",
        "        if len(new_lrs) != len(self.optimizer.param_groups):\n",
        "            raise ValueError(\n",
        "                \"Length of `new_lrs` is not equal to the number of parameter groups \"\n",
        "                + \"in the given optimizer\"\n",
        "            )\n",
        "\n",
        "        for param_group, new_lr in zip(self.optimizer.param_groups, new_lrs):\n",
        "            param_group[\"lr\"] = new_lr\n",
        "\n",
        "    def _train_on_batch(self, iter_wrapper, accumulation_steps):\n",
        "        self.model.train()\n",
        "        total_loss = None  # for late initialization\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        for i in range(accumulation_steps):\n",
        "            inputs, labels = iter_wrapper.get_batch()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.model(inputs)\n",
        "            loss = F.nll_loss(outputs, labels)\n",
        "\n",
        "            # Loss should be averaged in each step\n",
        "            loss /= accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if total_loss is None:\n",
        "                total_loss = loss.item()\n",
        "            else:\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def plot(self, skip_start=10, skip_end=5, log_lr=True, show_lr=None):\n",
        "        if skip_start < 0:\n",
        "            raise ValueError(\"skip_start cannot be negative\")\n",
        "        if skip_end < 0:\n",
        "            raise ValueError(\"skip_end cannot be negative\")\n",
        "        if show_lr is not None and not isinstance(show_lr, float):\n",
        "            raise ValueError(\"show_lr must be float\")\n",
        "\n",
        "        # Get the data to plot from the history dictionary. Also, handle skip_end=0\n",
        "        # properly so the behaviour is the expected\n",
        "        lrs = self.history[\"lr\"]\n",
        "        losses = self.history[\"loss\"]\n",
        "        if skip_end == 0:\n",
        "            lrs = lrs[skip_start:]\n",
        "            losses = losses[skip_start:]\n",
        "        else:\n",
        "            lrs = lrs[skip_start:-skip_end]\n",
        "            losses = losses[skip_start:-skip_end]\n",
        "\n",
        "        # Plot loss as a function of the learning rate\n",
        "        plt.plot(lrs, losses)\n",
        "        if log_lr:\n",
        "            plt.xscale(\"log\")\n",
        "        plt.xlabel(\"Learning rate\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "\n",
        "        if show_lr is not None:\n",
        "            plt.axvline(x=show_lr, color=\"red\")\n",
        "        plt.show()\n",
        "\n",
        "    def get_best_lr(self):\n",
        "      lrs = self.history['lr']\n",
        "      losses = self.history['loss']\n",
        "      return lrs[losses.index(min(losses))]\n",
        "\n",
        "class ExponentialLR(_LRScheduler):\n",
        "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
        "        self.end_lr = end_lr\n",
        "        self.num_iter = num_iter\n",
        "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        curr_iter = self.last_epoch + 1\n",
        "        r = curr_iter / self.num_iter\n",
        "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
        "\n",
        "class DataLoaderIterWrapper(object):\n",
        "    def __init__(self, data_loader, auto_reset=True):\n",
        "        self.data_loader = data_loader\n",
        "        self.auto_reset = auto_reset\n",
        "        self._iterator = iter(data_loader)\n",
        "\n",
        "    def __next__(self):\n",
        "        # Get a new set of inputs and labels\n",
        "        try:\n",
        "            inputs, labels = next(self._iterator)\n",
        "        except StopIteration:\n",
        "            if not self.auto_reset:\n",
        "                raise\n",
        "            self._iterator = iter(self.data_loader)\n",
        "            inputs, labels = next(self._iterator)\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def get_batch(self):\n",
        "        return next(self)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM_NKA9ZlHKv"
      },
      "source": [
        "## Train-Test loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Cv0XLX54Xo"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, steps_per_epoch=20):\n",
        "  # Switch model to training mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
        "  model.train()\n",
        "  train_total = 0\n",
        "  train_correct = 0\n",
        "\n",
        "  # We loop over the data iterator, and feed the inputs to the network and adjust the weights.\n",
        "  for batch_idx, (data, target) in enumerate(train_loader, start=0):\n",
        "    if batch_idx > steps_per_epoch:\n",
        "      break\n",
        "    # Load the input features and labels from the training dataset\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    \n",
        "    # Reset the gradients to 0 for all learnable weight parameters\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass: Pass image data from training dataset, make predictions about class image belongs to (0-9 in this case)\n",
        "    output = model(data)\n",
        "    \n",
        "    # Define our loss function, and compute the loss\n",
        "    loss = F.nll_loss(output, target)\n",
        "\n",
        "    scores, predictions = torch.max(output.data, 1)\n",
        "    train_total += target.size(0)\n",
        "    train_correct += int(sum(predictions == target))\n",
        "            \n",
        "    # Backward pass: compute the gradients of the loss w.r.t. the model's parameters\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update the neural network weights\n",
        "    optimizer.step()\n",
        "\n",
        "  acc = round((train_correct / train_total) * 100, 2)\n",
        "  print('Epoch [{}], Loss: {}, Accuracy: {}, '.format(epoch, loss.item(), acc), end='')\n",
        "  wandb.log({'Train Loss': loss.item(), 'Train Accuracy': acc})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHcCrrPJ65p-"
      },
      "source": [
        "def test(model, device, test_loader, classes):\n",
        "  # Switch model to evaluation mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
        "  model.eval()\n",
        "  \n",
        "  test_loss = 0\n",
        "  test_total = 0\n",
        "  test_correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          # Load the input features and labels from the test dataset\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          \n",
        "          # Make predictions: Pass image data from test dataset, make predictions about class image belongs to (0-9 in this case)\n",
        "          output = model(data)\n",
        "          \n",
        "          # Compute the loss sum up batch loss\n",
        "          test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "          \n",
        "          scores, predictions = torch.max(output.data, 1)\n",
        "          test_total += target.size(0)\n",
        "          test_correct += int(sum(predictions == target))\n",
        "          \n",
        "  acc = round((test_correct / test_total) * 100, 2)\n",
        "  print(' Test_loss: {}, Test_accuracy: {}'.format(test_loss/test_total, acc))\n",
        "  wandb.log({'Test Loss': test_loss/test_total, 'Test Accuracy': acc})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYOD_TUakTgC"
      },
      "source": [
        "## W&B and Model Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh8iMiyOkSvX"
      },
      "source": [
        "wandb.init(project='lrfinder')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCv0BZIUDd81"
      },
      "source": [
        "net = Net().to(device)\n",
        "print(net)\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArPLSklZJqu4"
      },
      "source": [
        "## LR Finder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MF-9qyBkQ13"
      },
      "source": [
        "lr_finder = LRFinder(net, optimizer, device)\n",
        "lr_finder.range_test(trainloader, end_lr=10, num_iter=100, logwandb=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYTWATxUkyYH"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVGiu_9hJv9_"
      },
      "source": [
        "del net\n",
        "net = Net().to(device)\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "wandb.watch(net, log='all')\n",
        "\n",
        "for epoch in range(10):\n",
        "  train(net, device, trainloader, optimizer, epoch)\n",
        "  test(net, device, testloader, classes)\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}